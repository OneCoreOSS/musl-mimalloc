diff -urN a/Makefile b/Makefile
--- a/Makefile	2025-07-15 11:19:20.086367069 -0400
+++ b/Makefile	2025-07-15 11:19:32.030931047 -0400
@@ -23,6 +23,7 @@
 ARCH_GLOBS = $(addsuffix /$(ARCH)/*.[csS],$(SRC_DIRS))
 BASE_SRCS = $(sort $(wildcard $(BASE_GLOBS)))
 ARCH_SRCS = $(sort $(wildcard $(ARCH_GLOBS)))
+MALLOC_OBJS = $(sort $(wildcard $(srcdir)/src/malloc/$(MALLOC_DIR)/*.o))
 BASE_OBJS = $(patsubst $(srcdir)/%,%.o,$(basename $(BASE_SRCS)))
 ARCH_OBJS = $(patsubst $(srcdir)/%,%.o,$(basename $(ARCH_SRCS)))
 REPLACED_OBJS = $(sort $(subst /$(ARCH)/,/,$(ARCH_OBJS)))
@@ -32,8 +33,8 @@
 LDSO_OBJS = $(filter obj/ldso/%,$(ALL_OBJS:%.o=%.lo))
 CRT_OBJS = $(filter obj/crt/%,$(ALL_OBJS))
 
-AOBJS = $(LIBC_OBJS)
-LOBJS = $(LIBC_OBJS:.o=.lo)
+AOBJS = $(LIBC_OBJS) $(MALLOC_OBJS)
+LOBJS = $(LIBC_OBJS:.o=.lo) $(MALLOC_OBJS)
 GENH = obj/include/bits/alltypes.h obj/include/bits/syscall.h
 GENH_INT = obj/src/internal/version.h
 IMPH = $(addprefix $(srcdir)/, src/internal/stdio_impl.h src/internal/pthread_impl.h src/internal/locale_impl.h src/internal/libc.h)
@@ -78,6 +79,8 @@
 -include config.mak
 -include $(srcdir)/arch/$(ARCH)/arch.mak
 
+obj/ldso/dlstart.lo: CFLAGS_ALL += -fno-lto
+
 ifeq ($(ARCH),)
 
 all:
@@ -127,10 +130,15 @@
 	, $(LIBC_OBJS))
 $(NOSSP_OBJS) $(NOSSP_OBJS:%.o=%.lo): CFLAGS_ALL += $(CFLAGS_NOSSP)
 
-$(CRT_OBJS): CFLAGS_ALL += -DCRT
+$(CRT_OBJS): CFLAGS_ALL += -DCRT -fno-lto
 
 $(LOBJS) $(LDSO_OBJS): CFLAGS_ALL += -fPIC
 
+ifneq (mallocng,$(MALLOC_DIR))
+obj/src/malloc/calloc.lo: CFLAGS_ALL += -DLIBC_CALLOC_EXTERNAL
+obj/src/malloc/libc_calloc.lo: CFLAGS_ALL += -DLIBC_CALLOC_EXTERNAL
+endif
+
 CC_CMD = $(CC) $(CFLAGS_ALL) -c -o $@ $<
 
 # Choose invocation of assembler to be used
@@ -140,6 +148,10 @@
 	AS_CMD = $(CC_CMD)
 endif
 
+$(EXTRA_OBJ): $(GENH) $(IMPH)
+	$(CC) -I$(srcdir)/mimalloc/include $(CFLAGS_ALL) -std=gnu11 -fPIC -O3 -DNDEBUG -fvisibility=hidden -isystem `$(CC) -print-search-dirs | grep ^libraries: |cut -d= -f2 |cut -d: -f1`/include -c -o $(EXTRA_OBJ) $(srcdir)/mimalloc/src/mimalloc.c
+	sh $(srcdir)/mimalloc-verify-syms.sh $(EXTRA_OBJ)
+
 obj/%.o: $(srcdir)/%.s
 	$(AS_CMD)
 
@@ -158,11 +170,11 @@
 obj/%.lo: $(srcdir)/%.c $(GENH) $(IMPH)
 	$(CC_CMD)
 
-lib/libc.so: $(LOBJS) $(LDSO_OBJS)
+lib/libc.so: $(LOBJS) $(LDSO_OBJS) $(EXTRA_OBJ)
 	$(CC) $(CFLAGS_ALL) $(LDFLAGS_ALL) -nostdlib -shared \
 	-Wl,-e,_dlstart -o $@ $(LOBJS) $(LDSO_OBJS) $(LIBCC)
 
-lib/libc.a: $(AOBJS)
+lib/libc.a: $(AOBJS) $(EXTRA_OBJ)
 	rm -f $@
 	$(AR) rc $@ $(AOBJS)
 	$(RANLIB) $@
diff -urN a/README.md b/README.md
--- a/README.md	1969-12-31 19:00:00.000000000 -0500
+++ b/README.md	2025-07-15 11:19:32.031047403 -0400
@@ -0,0 +1,31 @@
+# musl-mimalloc
+
+This repository hosts a patched version of the musl library that takes adventage of the mimalloc allocator.
+
+musl-mimalloc allows us to compile the world's fastest and most performant binaries in the Linux space.
+
+### 1. Clone the repository
+
+```bash
+$ git clone https://github.com/OneCoreOSS/musl-mimalloc
+$ cd musl-mimalloc
+```
+
+### 2. Compile musl
+
+```bash
+$ ./configure --with-malloc=external
+$ EXTRA_OBJ=$PWD/src/malloc/external/mimalloc.o make
+```
+
+Once completed, you should now have functional copy of musl-mimalloc in lib/libc.so.
+
+You can now install musl on your host and use it for your own projects.
+
+### Credits
+
+[musl libc](https://musl.libc.org/)
+
+[mimalloc](https://github.com/microsoft/mimalloc/)
+
+[Chimera Linux](https://github.com/chimera-linux/cports/tree/master/main/musl/) - provided all of the patches to make this work !
diff -urN a/arch/loongarch64/reloc.h b/arch/loongarch64/reloc.h
--- a/arch/loongarch64/reloc.h	2025-07-15 11:19:20.093927891 -0400
+++ b/arch/loongarch64/reloc.h	2025-07-15 11:19:32.037940331 -0400
@@ -17,6 +17,7 @@
 #define REL_TPOFF       R_LARCH_TLS_TPREL64
 #define REL_RELATIVE    R_LARCH_RELATIVE
 #define REL_SYMBOLIC    R_LARCH_64
+#define REL_TLSDESC     R_LARCH_TLS_DESC64
 
 #define CRTJMP(pc,sp) __asm__ __volatile__( \
 	"move $sp, %1 ; jr %0" : : "r"(pc), "r"(sp) : "memory" )
diff -urN a/arch/powerpc64/bits/signal.h b/arch/powerpc64/bits/signal.h
--- a/arch/powerpc64/bits/signal.h	2025-07-15 11:19:20.105622137 -0400
+++ b/arch/powerpc64/bits/signal.h	2025-07-15 11:19:32.049696107 -0400
@@ -2,8 +2,8 @@
  || defined(_XOPEN_SOURCE) || defined(_GNU_SOURCE) || defined(_BSD_SOURCE)
 
 #if defined(_XOPEN_SOURCE) || defined(_GNU_SOURCE) || defined(_BSD_SOURCE)
-#define MINSIGSTKSZ 4096
-#define SIGSTKSZ    10240
+#define MINSIGSTKSZ 8192
+#define SIGSTKSZ    32768
 #endif
 
 #if defined(_GNU_SOURCE) || defined(_BSD_SOURCE)
diff -urN a/arch/riscv64/bits/syscall.h.in b/arch/riscv64/bits/syscall.h.in
--- a/arch/riscv64/bits/syscall.h.in	2025-07-15 11:19:20.107895964 -0400
+++ b/arch/riscv64/bits/syscall.h.in	2025-07-15 11:19:32.052256494 -0400
@@ -307,3 +307,4 @@
 
 #define __NR_sysriscv __NR_arch_specific_syscall
 #define __NR_riscv_flush_icache (__NR_sysriscv + 15)
+#define __NR_riscv_hwprobe (__NR_sysriscv + 14)
diff -urN a/configure b/configure
--- a/configure	2025-07-15 11:19:20.117088910 -0400
+++ b/configure	2025-07-15 11:19:32.062114539 -0400
@@ -615,12 +615,7 @@
 tryldflag LDFLAGS_AUTO -Wl,--dynamic-list="$srcdir/dynamic.list"
 
 # Find compiler runtime library
-test -z "$LIBCC" && tryldflag LIBCC -lgcc && tryldflag LIBCC -lgcc_eh
-test -z "$LIBCC" && tryldflag LIBCC -lcompiler_rt
-test -z "$LIBCC" && try_libcc=`$CC -print-libgcc-file-name 2>/dev/null` \
-                 && tryldflag LIBCC "$try_libcc"
-test -z "$LIBCC" && try_libcc=`$CC -print-file-name=libpcc.a 2>/dev/null` \
-                 && tryldflag LIBCC "$try_libcc"
+test -z "$LIBCC" && LIBCC=`$CC -print-libgcc-file-name 2>/dev/null`
 printf "using compiler runtime libraries: %s\n" "$LIBCC"
 
 # Figure out arch variants for archs with variants
diff -urN a/include/elf.h b/include/elf.h
--- a/include/elf.h	2025-07-15 11:19:20.120474542 -0400
+++ b/include/elf.h	2025-07-15 11:19:32.065363911 -0400
@@ -3329,6 +3329,7 @@
 #define R_LARCH_TLS_TPREL32                 10
 #define R_LARCH_TLS_TPREL64                 11
 #define R_LARCH_IRELATIVE                   12
+#define R_LARCH_TLS_DESC64                  14
 #define R_LARCH_MARK_LA                     20
 #define R_LARCH_MARK_PCREL                  21
 #define R_LARCH_SOP_PUSH_PCREL              22
diff -urN a/ldso/dynlink.c b/ldso/dynlink.c
--- a/ldso/dynlink.c	2025-07-15 11:19:20.128006869 -0400
+++ b/ldso/dynlink.c	2025-07-15 11:19:32.074169726 -0400
@@ -1494,6 +1494,7 @@
 			fpaddr(p, dyn[DT_FINI])();
 #endif
 	}
+	__malloc_tls_teardown(self);
 }
 
 void __ldso_atfork(int who)
@@ -1828,6 +1829,9 @@
 	/* Activate error handler function */
 	error = error_impl;
 
+	/* Here we can initialize the allocator */
+	__malloc_init(__pthread_self());
+
 	/* If the main program was already loaded by the kernel,
 	 * AT_PHDR will point to some location other than the dynamic
 	 * linker's program headers. */
@@ -2031,9 +2035,12 @@
 	/* Actual copying to new TLS needs to happen after relocations,
 	 * since the TLS images might have contained relocated addresses. */
 	if (initial_tls != builtin_tls) {
-		if (__init_tp(__copy_tls(initial_tls)) < 0) {
+		void *mtls = __pthread_self()->malloc_tls;
+		pthread_t ns = __copy_tls(initial_tls);
+		if (__init_tp(ns) < 0) {
 			a_crash();
 		}
+		ns->malloc_tls = mtls;
 	} else {
 		size_t tmp_tls_size = libc.tls_size;
 		pthread_t self = __pthread_self();
diff -urN a/mimalloc/include/mimalloc/atomic.h b/mimalloc/include/mimalloc/atomic.h
--- a/mimalloc/include/mimalloc/atomic.h	2025-07-15 11:19:20.286693890 -0400
+++ b/mimalloc/include/mimalloc/atomic.h	2025-07-15 11:19:32.332601250 -0400
@@ -480,7 +480,7 @@
 
 #elif defined(MI_USE_PTHREADS)
 
-void _mi_error_message(int err, const char* fmt, ...);
+mi_decl_internal void _mi_error_message(int err, const char* fmt, ...);
 
 #define mi_lock_t  pthread_mutex_t
 
diff -urN a/mimalloc/include/mimalloc/internal.h b/mimalloc/include/mimalloc/internal.h
--- a/mimalloc/include/mimalloc/internal.h	2025-07-15 11:19:20.286808360 -0400
+++ b/mimalloc/include/mimalloc/internal.h	2025-07-15 11:19:32.332707269 -0400
@@ -13,6 +13,12 @@
 // functions and macros.
 // --------------------------------------------------------------------------
 
+#ifdef MI_LIBC_BUILD
+#define mi_decl_internal static
+#else
+#define mi_decl_internal extern
+#endif
+
 #include "types.h"
 #include "track.h"
 
@@ -21,6 +27,7 @@
 // Compiler defines
 // --------------------------------------------------------------------------
 
+
 #if (MI_DEBUG>0)
 #define mi_trace_message(...)  _mi_trace_message(__VA_ARGS__)
 #else
@@ -84,121 +91,120 @@
 #define __has_builtin(x)    0
 #endif
 
+#if defined(__EMSCRIPTEN__) && !defined(__wasi__)
+#define __wasi__
+#endif
+
 #if defined(__cplusplus)
 #define mi_decl_externc     extern "C"
 #else
 #define mi_decl_externc
 #endif
 
-#if defined(__EMSCRIPTEN__) && !defined(__wasi__)
-#define __wasi__
-#endif
-
-
 // --------------------------------------------------------------------------
 // Internal functions
 // --------------------------------------------------------------------------
 
 // "libc.c"
 #include    <stdarg.h>
-int         _mi_vsnprintf(char* buf, size_t bufsize, const char* fmt, va_list args);
-int         _mi_snprintf(char* buf, size_t buflen, const char* fmt, ...);
-char        _mi_toupper(char c);
-int         _mi_strnicmp(const char* s, const char* t, size_t n);
-void        _mi_strlcpy(char* dest, const char* src, size_t dest_size);
-void        _mi_strlcat(char* dest, const char* src, size_t dest_size);
-size_t      _mi_strlen(const char* s);
-size_t      _mi_strnlen(const char* s, size_t max_len);
-bool        _mi_getenv(const char* name, char* result, size_t result_size);
+mi_decl_internal int         _mi_vsnprintf(char* buf, size_t bufsize, const char* fmt, va_list args);
+mi_decl_internal int         _mi_snprintf(char* buf, size_t buflen, const char* fmt, ...);
+mi_decl_internal char        _mi_toupper(char c);
+mi_decl_internal int         _mi_strnicmp(const char* s, const char* t, size_t n);
+mi_decl_internal void        _mi_strlcpy(char* dest, const char* src, size_t dest_size);
+mi_decl_internal void        _mi_strlcat(char* dest, const char* src, size_t dest_size);
+mi_decl_internal size_t      _mi_strlen(const char* s);
+mi_decl_internal size_t      _mi_strnlen(const char* s, size_t max_len);
+mi_decl_internal bool        _mi_getenv(const char* name, char* result, size_t result_size);
 
 // "options.c"
-void        _mi_fputs(mi_output_fun* out, void* arg, const char* prefix, const char* message);
-void        _mi_fprintf(mi_output_fun* out, void* arg, const char* fmt, ...);
-void        _mi_message(const char* fmt, ...);
-void        _mi_warning_message(const char* fmt, ...);
-void        _mi_verbose_message(const char* fmt, ...);
-void        _mi_trace_message(const char* fmt, ...);
-void        _mi_options_init(void);
-long        _mi_option_get_fast(mi_option_t option);
-void        _mi_error_message(int err, const char* fmt, ...);
+mi_decl_internal void      _mi_fputs(mi_output_fun* out, void* arg, const char* prefix, const char* message);
+mi_decl_internal void      _mi_fprintf(mi_output_fun* out, void* arg, const char* fmt, ...);
+mi_decl_internal void      _mi_message(const char* fmt, ...);
+mi_decl_internal void      _mi_warning_message(const char* fmt, ...);
+mi_decl_internal void      _mi_verbose_message(const char* fmt, ...);
+mi_decl_internal void      _mi_trace_message(const char* fmt, ...);
+mi_decl_internal void      _mi_options_init(void);
+mi_decl_internal long      _mi_option_get_fast(mi_option_t option);
+mi_decl_internal void      _mi_error_message(int err, const char* fmt, ...);
 
 // random.c
-void        _mi_random_init(mi_random_ctx_t* ctx);
-void        _mi_random_init_weak(mi_random_ctx_t* ctx);
-void        _mi_random_reinit_if_weak(mi_random_ctx_t * ctx);
-void        _mi_random_split(mi_random_ctx_t* ctx, mi_random_ctx_t* new_ctx);
-uintptr_t   _mi_random_next(mi_random_ctx_t* ctx);
-uintptr_t   _mi_heap_random_next(mi_heap_t* heap);
-uintptr_t   _mi_os_random_weak(uintptr_t extra_seed);
-static inline uintptr_t _mi_random_shuffle(uintptr_t x);
+mi_decl_internal void                    _mi_random_init(mi_random_ctx_t* ctx);
+mi_decl_internal void                    _mi_random_init_weak(mi_random_ctx_t* ctx);
+mi_decl_internal void                    _mi_random_reinit_if_weak(mi_random_ctx_t * ctx);
+mi_decl_internal void                    _mi_random_split(mi_random_ctx_t* ctx, mi_random_ctx_t* new_ctx);
+mi_decl_internal uintptr_t               _mi_random_next(mi_random_ctx_t* ctx);
+mi_decl_internal uintptr_t               _mi_heap_random_next(mi_heap_t* heap);
+mi_decl_internal uintptr_t               _mi_os_random_weak(uintptr_t extra_seed);
+mi_decl_internal inline uintptr_t _mi_random_shuffle(uintptr_t x);
 
 // init.c
-extern mi_decl_hidden mi_decl_cache_align mi_stats_t       _mi_stats_main;
-extern mi_decl_hidden mi_decl_cache_align const mi_page_t  _mi_page_empty;
-void        _mi_auto_process_init(void);
-void mi_cdecl _mi_auto_process_done(void) mi_attr_noexcept;
-bool        _mi_is_redirected(void);
-bool        _mi_allocator_init(const char** message);
-void        _mi_allocator_done(void);
-bool        _mi_is_main_thread(void);
-size_t      _mi_current_thread_count(void);
-bool        _mi_preloading(void);           // true while the C runtime is not initialized yet
-void        _mi_thread_done(mi_heap_t* heap);
-void        _mi_thread_data_collect(void);
-void        _mi_tld_init(mi_tld_t* tld, mi_heap_t* bheap);
-mi_threadid_t _mi_thread_id(void) mi_attr_noexcept;
-mi_heap_t*    _mi_heap_main_get(void);     // statically allocated main backing heap
-mi_subproc_t* _mi_subproc_from_id(mi_subproc_id_t subproc_id);
-void        _mi_heap_guarded_init(mi_heap_t* heap);
+mi_decl_internal mi_decl_cache_align mi_stats_t       _mi_stats_main;
+mi_decl_internal mi_decl_cache_align const mi_page_t                 _mi_page_empty;
+mi_decl_internal void          _mi_auto_process_init(void);
+mi_decl_internal void mi_cdecl _mi_auto_process_done(void) mi_attr_noexcept;
+mi_decl_internal bool          _mi_is_redirected(void);
+mi_decl_internal bool          _mi_allocator_init(const char** message);
+mi_decl_internal void          _mi_allocator_done(void);
+mi_decl_internal bool          _mi_is_main_thread(void);
+mi_decl_internal size_t        _mi_current_thread_count(void);
+mi_decl_internal bool          _mi_preloading(void);           // true while the C runtime is not initialized yet
+mi_decl_internal void          _mi_thread_done(mi_heap_t* heap);
+mi_decl_internal void          _mi_thread_data_collect(void);
+mi_decl_internal void          _mi_tld_init(mi_tld_t* tld, mi_heap_t* bheap);
+mi_decl_internal mi_threadid_t _mi_thread_id(void) mi_attr_noexcept;
+mi_decl_internal mi_heap_t*    _mi_heap_main_get(void);     // statically allocated main backing heap
+mi_decl_internal mi_subproc_t* _mi_subproc_from_id(mi_subproc_id_t subproc_id);
+mi_decl_internal void          _mi_heap_guarded_init(mi_heap_t* heap);
 
 // os.c
-void        _mi_os_init(void);                                            // called from process init
-void*       _mi_os_alloc(size_t size, mi_memid_t* memid);
-void*       _mi_os_zalloc(size_t size, mi_memid_t* memid);
-void        _mi_os_free(void* p, size_t size, mi_memid_t memid);
-void        _mi_os_free_ex(void* p, size_t size, bool still_committed, mi_memid_t memid);
-
-size_t      _mi_os_page_size(void);
-size_t      _mi_os_good_alloc_size(size_t size);
-bool        _mi_os_has_overcommit(void);
-bool        _mi_os_has_virtual_reserve(void);
-
-bool        _mi_os_reset(void* addr, size_t size);
-bool        _mi_os_decommit(void* addr, size_t size);
-bool        _mi_os_unprotect(void* addr, size_t size);
-bool        _mi_os_purge(void* p, size_t size);
-bool        _mi_os_purge_ex(void* p, size_t size, bool allow_reset, size_t stat_size);
-void        _mi_os_reuse(void* p, size_t size);
-mi_decl_nodiscard bool _mi_os_commit(void* p, size_t size, bool* is_zero);
-mi_decl_nodiscard bool _mi_os_commit_ex(void* addr, size_t size, bool* is_zero, size_t stat_size);
-bool        _mi_os_protect(void* addr, size_t size);
-
-void*       _mi_os_alloc_aligned(size_t size, size_t alignment, bool commit, bool allow_large, mi_memid_t* memid);
-void*       _mi_os_alloc_aligned_at_offset(size_t size, size_t alignment, size_t align_offset, bool commit, bool allow_large, mi_memid_t* memid);
-
-void*       _mi_os_get_aligned_hint(size_t try_alignment, size_t size);
-bool        _mi_os_use_large_page(size_t size, size_t alignment);
-size_t      _mi_os_large_page_size(void);
-void*       _mi_os_alloc_huge_os_pages(size_t pages, int numa_node, mi_msecs_t max_secs, size_t* pages_reserved, size_t* psize, mi_memid_t* memid);
+mi_decl_internal void      _mi_os_init(void);                                            // called from process init
+mi_decl_internal void*     _mi_os_alloc(size_t size, mi_memid_t* memid);
+mi_decl_internal void*     _mi_os_zalloc(size_t size, mi_memid_t* memid);
+mi_decl_internal void      _mi_os_free(void* p, size_t size, mi_memid_t memid);
+mi_decl_internal void      _mi_os_free_ex(void* p, size_t size, bool still_committed, mi_memid_t memid);
+
+mi_decl_internal size_t      _mi_os_page_size(void);
+mi_decl_internal size_t      _mi_os_good_alloc_size(size_t size);
+mi_decl_internal bool        _mi_os_has_overcommit(void);
+mi_decl_internal bool        _mi_os_has_virtual_reserve(void);
+
+mi_decl_internal bool        _mi_os_reset(void* addr, size_t size);
+mi_decl_internal bool        _mi_os_decommit(void* addr, size_t size);
+mi_decl_internal bool        _mi_os_unprotect(void* addr, size_t size);
+mi_decl_internal bool        _mi_os_purge(void* p, size_t size);
+mi_decl_internal bool        _mi_os_purge_ex(void* p, size_t size, bool allow_reset, size_t stat_size);
+mi_decl_internal void        _mi_os_reuse(void* p, size_t size);
+mi_decl_internal bool _mi_os_commit(void* p, size_t size, bool* is_zero);
+mi_decl_internal bool _mi_os_commit_ex(void* addr, size_t size, bool* is_zero, size_t stat_size);
+mi_decl_internal bool        _mi_os_protect(void* addr, size_t size);
+
+mi_decl_internal void*       _mi_os_alloc_aligned(size_t size, size_t alignment, bool commit, bool allow_large, mi_memid_t* memid);
+mi_decl_internal void*       _mi_os_alloc_aligned_at_offset(size_t size, size_t alignment, size_t align_offset, bool commit, bool allow_large, mi_memid_t* memid);
+
+mi_decl_internal void*       _mi_os_get_aligned_hint(size_t try_alignment, size_t size);
+mi_decl_internal bool        _mi_os_use_large_page(size_t size, size_t alignment);
+mi_decl_internal size_t      _mi_os_large_page_size(void);
+mi_decl_internal void*       _mi_os_alloc_huge_os_pages(size_t pages, int numa_node, mi_msecs_t max_secs, size_t* pages_reserved, size_t* psize, mi_memid_t* memid);
 
-int         _mi_os_numa_node_count(void);
-int         _mi_os_numa_node(void);
+mi_decl_internal int         _mi_os_numa_node_count(void);
+mi_decl_internal int         _mi_os_numa_node(void);
 
 // arena.c
-mi_arena_id_t _mi_arena_id_none(void);
-void        _mi_arena_free(void* p, size_t size, size_t still_committed_size, mi_memid_t memid);
-void*       _mi_arena_alloc(size_t size, bool commit, bool allow_large, mi_arena_id_t req_arena_id, mi_memid_t* memid);
-void*       _mi_arena_alloc_aligned(size_t size, size_t alignment, size_t align_offset, bool commit, bool allow_large, mi_arena_id_t req_arena_id, mi_memid_t* memid);
-bool        _mi_arena_memid_is_suitable(mi_memid_t memid, mi_arena_id_t request_arena_id);
-bool        _mi_arena_contains(const void* p);
-void        _mi_arenas_collect(bool force_purge);
-void        _mi_arena_unsafe_destroy_all(void);
+mi_decl_internal mi_arena_id_t _mi_arena_id_none(void);
+mi_decl_internal void          _mi_arena_free(void* p, size_t size, size_t still_committed_size, mi_memid_t memid);
+mi_decl_internal void*         _mi_arena_alloc(size_t size, bool commit, bool allow_large, mi_arena_id_t req_arena_id, mi_memid_t* memid);
+mi_decl_internal void*         _mi_arena_alloc_aligned(size_t size, size_t alignment, size_t align_offset, bool commit, bool allow_large, mi_arena_id_t req_arena_id, mi_memid_t* memid);
+mi_decl_internal bool          _mi_arena_memid_is_suitable(mi_memid_t memid, mi_arena_id_t request_arena_id);
+mi_decl_internal bool          _mi_arena_contains(const void* p);
+mi_decl_internal void          _mi_arenas_collect(bool force_purge);
+mi_decl_internal void          _mi_arena_unsafe_destroy_all(void);
 
-bool        _mi_arena_segment_clear_abandoned(mi_segment_t* segment);
-void        _mi_arena_segment_mark_abandoned(mi_segment_t* segment);
+mi_decl_internal bool        _mi_arena_segment_clear_abandoned(mi_segment_t* segment);
+mi_decl_internal void        _mi_arena_segment_mark_abandoned(mi_segment_t* segment);
 
-void*       _mi_arena_meta_zalloc(size_t size, mi_memid_t* memid);
-void        _mi_arena_meta_free(void* p, mi_memid_t memid, size_t size);
+mi_decl_internal void*       _mi_arena_meta_zalloc(size_t size, mi_memid_t* memid);
+mi_decl_internal void        _mi_arena_meta_free(void* p, mi_memid_t memid, size_t size);
 
 typedef struct mi_arena_field_cursor_s { // abstract struct
   size_t         os_list_count;           // max entries to visit in the OS abandoned list
@@ -209,91 +215,91 @@
   bool           visit_all;               // ensure all abandoned blocks are seen (blocking)
   bool           hold_visit_lock;         // if the subproc->abandoned_os_visit_lock is held
 } mi_arena_field_cursor_t;
-void          _mi_arena_field_cursor_init(mi_heap_t* heap, mi_subproc_t* subproc, bool visit_all, mi_arena_field_cursor_t* current);
-mi_segment_t* _mi_arena_segment_clear_abandoned_next(mi_arena_field_cursor_t* previous);
-void          _mi_arena_field_cursor_done(mi_arena_field_cursor_t* current);
+mi_decl_internal void          _mi_arena_field_cursor_init(mi_heap_t* heap, mi_subproc_t* subproc, bool visit_all, mi_arena_field_cursor_t* current);
+mi_decl_internal mi_segment_t* _mi_arena_segment_clear_abandoned_next(mi_arena_field_cursor_t* previous);
+mi_decl_internal void          _mi_arena_field_cursor_done(mi_arena_field_cursor_t* current);
 
 // "segment-map.c"
-void        _mi_segment_map_allocated_at(const mi_segment_t* segment);
-void        _mi_segment_map_freed_at(const mi_segment_t* segment);
-void        _mi_segment_map_unsafe_destroy(void);
+mi_decl_internal void      _mi_segment_map_allocated_at(const mi_segment_t* segment);
+mi_decl_internal void      _mi_segment_map_freed_at(const mi_segment_t* segment);
+mi_decl_internal void      _mi_segment_map_unsafe_destroy(void);
 
 // "segment.c"
-mi_page_t* _mi_segment_page_alloc(mi_heap_t* heap, size_t block_size, size_t page_alignment, mi_segments_tld_t* tld);
-void       _mi_segment_page_free(mi_page_t* page, bool force, mi_segments_tld_t* tld);
-void       _mi_segment_page_abandon(mi_page_t* page, mi_segments_tld_t* tld);
-bool       _mi_segment_try_reclaim_abandoned( mi_heap_t* heap, bool try_all, mi_segments_tld_t* tld);
-void       _mi_segment_collect(mi_segment_t* segment, bool force);
+mi_decl_internal mi_page_t* _mi_segment_page_alloc(mi_heap_t* heap, size_t block_size, size_t page_alignment, mi_segments_tld_t* tld);
+mi_decl_internal void       _mi_segment_page_free(mi_page_t* page, bool force, mi_segments_tld_t* tld);
+mi_decl_internal void       _mi_segment_page_abandon(mi_page_t* page, mi_segments_tld_t* tld);
+mi_decl_internal bool       _mi_segment_try_reclaim_abandoned( mi_heap_t* heap, bool try_all, mi_segments_tld_t* tld);
+mi_decl_internal void       _mi_segment_collect(mi_segment_t* segment, bool force);
 
 #if MI_HUGE_PAGE_ABANDON
-void        _mi_segment_huge_page_free(mi_segment_t* segment, mi_page_t* page, mi_block_t* block);
+mi_decl_internal void        _mi_segment_huge_page_free(mi_segment_t* segment, mi_page_t* page, mi_block_t* block);
 #else
-void        _mi_segment_huge_page_reset(mi_segment_t* segment, mi_page_t* page, mi_block_t* block);
+mi_decl_internal void        _mi_segment_huge_page_reset(mi_segment_t* segment, mi_page_t* page, mi_block_t* block);
 #endif
 
-uint8_t*   _mi_segment_page_start(const mi_segment_t* segment, const mi_page_t* page, size_t* page_size); // page start for any page
-void       _mi_abandoned_reclaim_all(mi_heap_t* heap, mi_segments_tld_t* tld);
-void       _mi_abandoned_collect(mi_heap_t* heap, bool force, mi_segments_tld_t* tld);
-bool       _mi_segment_attempt_reclaim(mi_heap_t* heap, mi_segment_t* segment);
-bool       _mi_segment_visit_blocks(mi_segment_t* segment, int heap_tag, bool visit_blocks, mi_block_visit_fun* visitor, void* arg);
+mi_decl_internal uint8_t*   _mi_segment_page_start(const mi_segment_t* segment, const mi_page_t* page, size_t* page_size); // page start for any page
+mi_decl_internal void       _mi_abandoned_reclaim_all(mi_heap_t* heap, mi_segments_tld_t* tld);
+mi_decl_internal void       _mi_abandoned_collect(mi_heap_t* heap, bool force, mi_segments_tld_t* tld);
+mi_decl_internal bool       _mi_segment_attempt_reclaim(mi_heap_t* heap, mi_segment_t* segment);
+mi_decl_internal bool       _mi_segment_visit_blocks(mi_segment_t* segment, int heap_tag, bool visit_blocks, mi_block_visit_fun* visitor, void* arg);
 
 // "page.c"
-void*       _mi_malloc_generic(mi_heap_t* heap, size_t size, bool zero, size_t huge_alignment)  mi_attr_noexcept mi_attr_malloc;
+mi_decl_internal void*     _mi_malloc_generic(mi_heap_t* heap, size_t size, bool zero, size_t huge_alignment)  mi_attr_noexcept mi_attr_malloc;
 
-void        _mi_page_retire(mi_page_t* page) mi_attr_noexcept;                  // free the page if there are no other pages with many free blocks
-void        _mi_page_unfull(mi_page_t* page);
-void        _mi_page_free(mi_page_t* page, mi_page_queue_t* pq, bool force);   // free the page
-void        _mi_page_abandon(mi_page_t* page, mi_page_queue_t* pq);            // abandon the page, to be picked up by another thread...
-void        _mi_page_force_abandon(mi_page_t* page);
-
-void        _mi_heap_delayed_free_all(mi_heap_t* heap);
-bool        _mi_heap_delayed_free_partial(mi_heap_t* heap);
-void        _mi_heap_collect_retired(mi_heap_t* heap, bool force);
-
-void        _mi_page_use_delayed_free(mi_page_t* page, mi_delayed_t delay, bool override_never);
-bool        _mi_page_try_use_delayed_free(mi_page_t* page, mi_delayed_t delay, bool override_never);
-size_t      _mi_page_queue_append(mi_heap_t* heap, mi_page_queue_t* pq, mi_page_queue_t* append);
-void        _mi_deferred_free(mi_heap_t* heap, bool force);
-
-void        _mi_page_free_collect(mi_page_t* page,bool force);
-void        _mi_page_reclaim(mi_heap_t* heap, mi_page_t* page);   // callback from segments
-
-size_t      _mi_page_bin(const mi_page_t* page); // for stats
-size_t      _mi_bin_size(size_t bin);            // for stats
-size_t      _mi_bin(size_t size);                // for stats
+mi_decl_internal void      _mi_page_retire(mi_page_t* page) mi_attr_noexcept;                  // free the page if there are no other pages with many free blocks
+mi_decl_internal void      _mi_page_unfull(mi_page_t* page);
+mi_decl_internal void      _mi_page_free(mi_page_t* page, mi_page_queue_t* pq, bool force);   // free the page
+mi_decl_internal void      _mi_page_abandon(mi_page_t* page, mi_page_queue_t* pq);            // abandon the page, to be picked up by another thread...
+mi_decl_internal void      _mi_page_force_abandon(mi_page_t* page);
+
+mi_decl_internal void      _mi_heap_delayed_free_all(mi_heap_t* heap);
+mi_decl_internal bool      _mi_heap_delayed_free_partial(mi_heap_t* heap);
+mi_decl_internal void      _mi_heap_collect_retired(mi_heap_t* heap, bool force);
+
+mi_decl_internal void      _mi_page_use_delayed_free(mi_page_t* page, mi_delayed_t delay, bool override_never);
+mi_decl_internal bool      _mi_page_try_use_delayed_free(mi_page_t* page, mi_delayed_t delay, bool override_never);
+mi_decl_internal size_t    _mi_page_queue_append(mi_heap_t* heap, mi_page_queue_t* pq, mi_page_queue_t* append);
+mi_decl_internal void      _mi_deferred_free(mi_heap_t* heap, bool force);
+
+mi_decl_internal void      _mi_page_free_collect(mi_page_t* page,bool force);
+mi_decl_internal void      _mi_page_reclaim(mi_heap_t* heap, mi_page_t* page);   // callback from segments
+
+mi_decl_internal size_t    _mi_page_bin(const mi_page_t* page); // for stats
+mi_decl_internal size_t    _mi_bin_size(size_t bin);            // for stats
+mi_decl_internal size_t    _mi_bin(size_t size);                // for stats
 
 // "heap.c"
-void        _mi_heap_init(mi_heap_t* heap, mi_tld_t* tld, mi_arena_id_t arena_id, bool noreclaim, uint8_t tag);
-void        _mi_heap_destroy_pages(mi_heap_t* heap);
-void        _mi_heap_collect_abandon(mi_heap_t* heap);
-void        _mi_heap_set_default_direct(mi_heap_t* heap);
-bool        _mi_heap_memid_is_suitable(mi_heap_t* heap, mi_memid_t memid);
-void        _mi_heap_unsafe_destroy_all(mi_heap_t* heap);
-mi_heap_t*  _mi_heap_by_tag(mi_heap_t* heap, uint8_t tag);
-void        _mi_heap_area_init(mi_heap_area_t* area, mi_page_t* page);
-bool        _mi_heap_area_visit_blocks(const mi_heap_area_t* area, mi_page_t* page, mi_block_visit_fun* visitor, void* arg);
+mi_decl_internal void        _mi_heap_init(mi_heap_t* heap, mi_tld_t* tld, mi_arena_id_t arena_id, bool noreclaim, uint8_t tag);
+mi_decl_internal void        _mi_heap_destroy_pages(mi_heap_t* heap);
+mi_decl_internal void        _mi_heap_collect_abandon(mi_heap_t* heap);
+mi_decl_internal void        _mi_heap_set_default_direct(mi_heap_t* heap);
+mi_decl_internal bool        _mi_heap_memid_is_suitable(mi_heap_t* heap, mi_memid_t memid);
+mi_decl_internal void        _mi_heap_unsafe_destroy_all(mi_heap_t* heap);
+mi_decl_internal mi_heap_t*  _mi_heap_by_tag(mi_heap_t* heap, uint8_t tag);
+mi_decl_internal void        _mi_heap_area_init(mi_heap_area_t* area, mi_page_t* page);
+mi_decl_internal bool        _mi_heap_area_visit_blocks(const mi_heap_area_t* area, mi_page_t* page, mi_block_visit_fun* visitor, void* arg);
 
 // "stats.c"
-void        _mi_stats_done(mi_stats_t* stats);
-void        _mi_stats_merge_thread(mi_tld_t* tld);
-mi_msecs_t  _mi_clock_now(void);
-mi_msecs_t  _mi_clock_end(mi_msecs_t start);
-mi_msecs_t  _mi_clock_start(void);
+mi_decl_internal void        _mi_stats_done(mi_stats_t* stats);
+mi_decl_internal void        _mi_stats_merge_thread(mi_tld_t* tld);
+mi_decl_internal mi_msecs_t  _mi_clock_now(void);
+mi_decl_internal mi_msecs_t  _mi_clock_end(mi_msecs_t start);
+mi_decl_internal mi_msecs_t  _mi_clock_start(void);
 
 // "alloc.c"
-void*       _mi_page_malloc_zero(mi_heap_t* heap, mi_page_t* page, size_t size, bool zero) mi_attr_noexcept;  // called from `_mi_malloc_generic`
-void*       _mi_page_malloc(mi_heap_t* heap, mi_page_t* page, size_t size) mi_attr_noexcept;                  // called from `_mi_heap_malloc_aligned`
-void*       _mi_page_malloc_zeroed(mi_heap_t* heap, mi_page_t* page, size_t size) mi_attr_noexcept;           // called from `_mi_heap_malloc_aligned`
-void*       _mi_heap_malloc_zero(mi_heap_t* heap, size_t size, bool zero) mi_attr_noexcept;
-void*       _mi_heap_malloc_zero_ex(mi_heap_t* heap, size_t size, bool zero, size_t huge_alignment) mi_attr_noexcept;     // called from `_mi_heap_malloc_aligned`
-void*       _mi_heap_realloc_zero(mi_heap_t* heap, void* p, size_t newsize, bool zero) mi_attr_noexcept;
-mi_block_t* _mi_page_ptr_unalign(const mi_page_t* page, const void* p);
-bool        _mi_free_delayed_block(mi_block_t* block);
-void        _mi_free_generic(mi_segment_t* segment, mi_page_t* page, bool is_local, void* p) mi_attr_noexcept;  // for runtime integration
-void        _mi_padding_shrink(const mi_page_t* page, const mi_block_t* block, const size_t min_size);
+mi_decl_internal void*       _mi_page_malloc_zero(mi_heap_t* heap, mi_page_t* page, size_t size, bool zero) mi_attr_noexcept;  // called from `_mi_malloc_generic`
+mi_decl_internal void*       _mi_page_malloc(mi_heap_t* heap, mi_page_t* page, size_t size) mi_attr_noexcept;                  // called from `_mi_heap_malloc_aligned`
+mi_decl_internal void*       _mi_page_malloc_zeroed(mi_heap_t* heap, mi_page_t* page, size_t size) mi_attr_noexcept;           // called from `_mi_heap_malloc_aligned`
+mi_decl_internal void*       _mi_heap_malloc_zero(mi_heap_t* heap, size_t size, bool zero) mi_attr_noexcept;
+mi_decl_internal void*       _mi_heap_malloc_zero_ex(mi_heap_t* heap, size_t size, bool zero, size_t huge_alignment) mi_attr_noexcept;     // called from `_mi_heap_malloc_aligned`
+mi_decl_internal void*       _mi_heap_realloc_zero(mi_heap_t* heap, void* p, size_t newsize, bool zero) mi_attr_noexcept;
+mi_decl_internal mi_block_t* _mi_page_ptr_unalign(const mi_page_t* page, const void* p);
+mi_decl_internal bool        _mi_free_delayed_block(mi_block_t* block);
+mi_decl_internal void        _mi_free_generic(mi_segment_t* segment, mi_page_t* page, bool is_local, void* p) mi_attr_noexcept;  // for runtime integration
+mi_decl_internal void        _mi_padding_shrink(const mi_page_t* page, const mi_block_t* block, const size_t min_size);
 
 #if MI_DEBUG>1
-bool        _mi_page_is_valid(mi_page_t* page);
+mi_decl_internal bool        _mi_page_is_valid(mi_page_t* page);
 #endif
 
 
@@ -320,15 +326,14 @@
 #define EOVERFLOW (75)
 #endif
 
-
 // ------------------------------------------------------
 // Assertions
 // ------------------------------------------------------
 
 #if (MI_DEBUG)
 // use our own assertion to print without memory allocation
-mi_decl_noreturn mi_decl_cold void _mi_assert_fail(const char* assertion, const char* fname, unsigned int line, const char* func) mi_attr_noexcept;
-#define mi_assert(expr)     ((expr) ? (void)0 : _mi_assert_fail(#expr,__FILE__,__LINE__,__func__))
+mi_decl_noreturn mi_decl_cold void _mi_assert_fail(const char* assertion, const cha>
+#define mi_assert(expr)     ((expr) ? (void)0 : _mi_assert_fail(#expr,__FILE__,__LI>
 #else
 #define mi_assert(x)
 #endif
@@ -346,7 +351,6 @@
 #endif
 
 
-
 /* -----------------------------------------------------------
   Inlined definitions
 ----------------------------------------------------------- */
@@ -884,8 +888,8 @@
 }
 
 // defined in `segment.c`:
-size_t _mi_commit_mask_committed_size(const mi_commit_mask_t* cm, size_t total);
-size_t _mi_commit_mask_next_run(const mi_commit_mask_t* cm, size_t* idx);
+mi_decl_internal size_t _mi_commit_mask_committed_size(const mi_commit_mask_t* cm, size_t total);
+mi_decl_internal size_t _mi_commit_mask_next_run(const mi_commit_mask_t* cm, size_t* idx);
 
 #define mi_commit_mask_foreach(cm,idx,count) \
   idx = 0; \
@@ -1064,16 +1068,22 @@
   return (x==0 ? MI_SIZE_BITS : MI_SIZE_BITS - 1 - mi_clz(x));
 }
 
-size_t _mi_popcount_generic(size_t x);
+mi_decl_internal size_t _mi_popcount_generic(size_t x);
+
 
 static inline size_t mi_popcount(size_t x) {
   if (x<=1) return x;
   if (x==SIZE_MAX) return MI_SIZE_BITS;
   #if defined(__GNUC__)
     #if (SIZE_MAX == ULONG_MAX)
-      return __builtin_popcountl(x);
+      x -= (x >> 1) & 0x55555555;
+      x = (x & 0x33333333) + ((x >> 2) & 0x33333333);
+      return ((x + (x >> 4) & 0x0F0F0F0F) * 0x01010101) >> 24;
     #else
-      return __builtin_popcountll(x);
+      x -= (x >> 1) & 0x5555555555555555;
+      x = (x & 0x3333333333333333) + ((x >> 2) & 0x3333333333333333);
+      x = (x + (x >> 4)) & 0x0F0F0F0F0F0F0F0F;
+      return (x * 0x0101010101010101) >> 56;
     #endif
   #else
     return _mi_popcount_generic(x);
diff -urN a/mimalloc/include/mimalloc/prim.h b/mimalloc/include/mimalloc/prim.h
--- a/mimalloc/include/mimalloc/prim.h	2025-07-15 11:19:20.286886513 -0400
+++ b/mimalloc/include/mimalloc/prim.h	2025-07-15 11:19:32.332829352 -0400
@@ -8,6 +8,12 @@
 #ifndef MIMALLOC_PRIM_H
 #define MIMALLOC_PRIM_H
 
+#ifdef MI_LIBC_BUILD
+#define mi_prim_internal static
+#else
+#define mi_prim_internal extern
+#endif
+
 
 // --------------------------------------------------------------------------
 // This file specifies the primitive portability API.
@@ -33,10 +39,10 @@
 } mi_os_mem_config_t;
 
 // Initialize
-void _mi_prim_mem_init( mi_os_mem_config_t* config );
+mi_prim_internal void _mi_prim_mem_init( mi_os_mem_config_t* config );
 
 // Free OS memory
-int _mi_prim_free(void* addr, size_t size );
+mi_prim_internal int _mi_prim_free(void* addr, size_t size );
 
 // Allocate OS memory. Return NULL on error.
 // The `try_alignment` is just a hint and the returned pointer does not have to be aligned.
@@ -46,45 +52,45 @@
 // The `hint_addr` address is either `NULL` or a preferred allocation address but can be ignored.
 // pre: !commit => !allow_large
 //      try_alignment >= _mi_os_page_size() and a power of 2
-int _mi_prim_alloc(void* hint_addr, size_t size, size_t try_alignment, bool commit, bool allow_large, bool* is_large, bool* is_zero, void** addr);
+mi_prim_internal int _mi_prim_alloc(void* hint_addr, size_t size, size_t try_alignment, bool commit, bool allow_large, bool* is_large, bool* is_zero, void** addr);
 
 // Commit memory. Returns error code or 0 on success.
 // For example, on Linux this would make the memory PROT_READ|PROT_WRITE.
 // `is_zero` is set to true if the memory was zero initialized (e.g. on Windows)
-int _mi_prim_commit(void* addr, size_t size, bool* is_zero);
+mi_prim_internal int _mi_prim_commit(void* addr, size_t size, bool* is_zero);
 
 // Decommit memory. Returns error code or 0 on success. The `needs_recommit` result is true
 // if the memory would need to be re-committed. For example, on Windows this is always true,
 // but on Linux we could use MADV_DONTNEED to decommit which does not need a recommit.
 // pre: needs_recommit != NULL
-int _mi_prim_decommit(void* addr, size_t size, bool* needs_recommit);
+mi_prim_internal int _mi_prim_decommit(void* addr, size_t size, bool* needs_recommit);
 
 // Reset memory. The range keeps being accessible but the content might be reset to zero at any moment.
 // Returns error code or 0 on success.
-int _mi_prim_reset(void* addr, size_t size);
+mi_prim_internal int _mi_prim_reset(void* addr, size_t size);
 
 // Reuse memory. This is called for memory that is already committed but
 // may have been reset (`_mi_prim_reset`) or decommitted (`_mi_prim_decommit`) where `needs_recommit` was false.
 // Returns error code or 0 on success. On most platforms this is a no-op.
-int _mi_prim_reuse(void* addr, size_t size);
+mi_prim_internal int _mi_prim_reuse(void* addr, size_t size);
 
 // Protect memory. Returns error code or 0 on success.
-int _mi_prim_protect(void* addr, size_t size, bool protect);
+mi_prim_internal int _mi_prim_protect(void* addr, size_t size, bool protect);
 
 // Allocate huge (1GiB) pages possibly associated with a NUMA node.
 // `is_zero` is set to true if the memory was zero initialized (as on most OS's)
 // pre: size > 0  and a multiple of 1GiB.
 //      numa_node is either negative (don't care), or a numa node number.
-int _mi_prim_alloc_huge_os_pages(void* hint_addr, size_t size, int numa_node, bool* is_zero, void** addr);
+mi_prim_internal int _mi_prim_alloc_huge_os_pages(void* hint_addr, size_t size, int numa_node, bool* is_zero, void** addr);
 
 // Return the current NUMA node
-size_t _mi_prim_numa_node(void);
+mi_prim_internal size_t _mi_prim_numa_node(void);
 
 // Return the number of logical NUMA nodes
-size_t _mi_prim_numa_node_count(void);
+mi_prim_internal size_t _mi_prim_numa_node_count(void);
 
 // Clock ticks
-mi_msecs_t _mi_prim_clock_now(void);
+mi_prim_internal mi_msecs_t _mi_prim_clock_now(void);
 
 // Return process information (only for statistics)
 typedef struct mi_process_info_s {
@@ -98,29 +104,32 @@
   size_t      page_faults;
 } mi_process_info_t;
 
-void _mi_prim_process_info(mi_process_info_t* pinfo);
+mi_prim_internal void _mi_prim_process_info(mi_process_info_t* pinfo);
 
 // Default stderr output. (only for warnings etc. with verbose enabled)
 // msg != NULL && _mi_strlen(msg) > 0
-void _mi_prim_out_stderr( const char* msg );
+mi_prim_internal void _mi_prim_out_stderr( const char* msg );
 
 // Get an environment variable. (only for options)
 // name != NULL, result != NULL, result_size >= 64
-bool _mi_prim_getenv(const char* name, char* result, size_t result_size);
+mi_prim_internal bool _mi_prim_getenv(const char* name, char* result, size_t result_size);
 
 
 // Fill a buffer with strong randomness; return `false` on error or if
 // there is no strong randomization available.
-bool _mi_prim_random_buf(void* buf, size_t buf_len);
+mi_prim_internal bool _mi_prim_random_buf(void* buf, size_t buf_len);
 
 // Called on the first thread start, and should ensure `_mi_thread_done` is called on thread termination.
-void _mi_prim_thread_init_auto_done(void);
+mi_prim_internal void _mi_prim_thread_init_auto_done(void);
 
 // Called on process exit and may take action to clean up resources associated with the thread auto done.
-void _mi_prim_thread_done_auto_done(void);
+mi_prim_internal void _mi_prim_thread_done_auto_done(void);
 
 // Called when the default heap for a thread changes
-void _mi_prim_thread_associate_default_heap(mi_heap_t* heap);
+mi_prim_internal void _mi_prim_thread_associate_default_heap(mi_heap_t* heap);
+
+
+
 
 
 //-------------------------------------------------------------------
@@ -213,14 +222,14 @@
 // thread-local initialization checks in the fast path.
 // We allocate a user TLS slot at process initialization (see `windows/prim.c`)
 // and store the offset `_mi_win_tls_offset`.
-#define MI_HAS_TLS_SLOT  1              // 2 = we can reliably initialize the slot (saving a test on each malloc)
 
-extern mi_decl_hidden size_t _mi_win_tls_offset;
+#define MI_HAS_TLS_SLOT  1              // 2 = we can reliably initialize the slot (saving a test on each malloc)
+mi_decl_internal size_t _mi_win_tls_offset;
 
 #if MI_WIN_USE_FIXED_TLS > 1
 #define MI_TLS_SLOT     (MI_WIN_USE_FIXED_TLS)
 #elif MI_SIZE_SIZE == 4
-#define MI_TLS_SLOT     (0x0E10 + _mi_win_tls_offset)  // User TLS slots <https://en.wikipedia.org/wiki/Win32_Thread_Information_Block>
++#define MI_TLS_SLOT     (0x0E10 + _mi_win_tls_offset)  // User TLS slots <https://en.wikipedia.org/wiki/Win32_Thread_Information_Block>
 #else
 #define MI_TLS_SLOT     (0x1480 + _mi_win_tls_offset)  // User TLS slots <https://en.wikipedia.org/wiki/Win32_Thread_Information_Block>
 #endif
@@ -252,7 +261,7 @@
 // for each thread (unequal to zero).
 //-------------------------------------------------------------------
 
-
+#ifndef MI_LIBC_BUILD
 // Do we have __builtin_thread_pointer? This would be the preferred way to get a unique thread id
 // but unfortunately, it seems we cannot test for this reliably at this time (see issue #883)
 // Nevertheless, it seems needed on older graviton platforms (see issue #851).
@@ -267,12 +276,14 @@
     #define MI_USE_BUILTIN_THREAD_POINTER  1
   #endif
 #endif
-
+#endif
 
 
 // defined in `init.c`; do not use these directly
+#ifndef MI_LIBC_BUILD
 extern mi_decl_hidden mi_decl_thread mi_heap_t* _mi_heap_default;  // default heap to allocate from
-extern mi_decl_hidden bool _mi_process_is_initialized;             // has mi_process_init been called?
+#endif
+mi_prim_internal bool _mi_process_is_initialized;             // has mi_process_init been called?
 
 static inline mi_threadid_t _mi_prim_thread_id(void) mi_attr_noexcept;
 
@@ -312,6 +323,13 @@
   #endif
 }
 
+#elif defined(MI_LIBC_BUILD)
+
+// chimera musl
+static inline mi_threadid_t _mi_prim_thread_id(void) mi_attr_noexcept {
+  return __pthread_self()->tid;
+}
+
 #else
 
 // otherwise use portable C, taking the address of a thread local variable (this is still very fast on most platforms).
@@ -406,6 +424,14 @@
   return (mi_unlikely(heap == NULL) ? (mi_heap_t*)&_mi_heap_empty : heap);
 }
 
+#elif defined(MI_LIBC_BUILD)
+
+// chimera musl
+static inline mi_heap_t* mi_prim_get_default_heap(void) {
+  return __pthread_self()->malloc_tls;
+}
+
+
 #else // default using a thread local variable; used on most platforms.
 
 static inline mi_heap_t* mi_prim_get_default_heap(void) {
diff -urN a/mimalloc/include/mimalloc/types.h b/mimalloc/include/mimalloc/types.h
--- a/mimalloc/include/mimalloc/types.h	2025-07-15 11:19:20.286982685 -0400
+++ b/mimalloc/include/mimalloc/types.h	2025-07-15 11:19:32.332977974 -0400
@@ -571,6 +571,7 @@
   size_t                guarded_size_min;                    // minimal size for guarded objects
   size_t                guarded_size_max;                    // maximal size for guarded objects
   size_t                guarded_sample_rate;                 // sample rate (set to 0 to disable guarded pages)
+  size_t                guarded_sample_seed;                 // starting sample count
   size_t                guarded_sample_count;                // current sample count (counting down to 0)
   #endif
   mi_page_t*            pages_free_direct[MI_PAGES_DIRECT];  // optimize: array where every entry points a page with possibly free blocks in the corresponding queue for that size.
@@ -632,6 +633,7 @@
 };
 
 
+
 // ------------------------------------------------------
 // Debug
 // ------------------------------------------------------
@@ -646,6 +648,26 @@
 #define MI_DEBUG_PADDING    (0xDE)
 #endif
 
+#if (MI_DEBUG)
+// use our own assertion to print without memory allocation
+void _mi_assert_fail(const char* assertion, const char* fname, unsigned int line, const char* func );
+#define mi_assert(expr)     ((expr) ? (void)0 : _mi_assert_fail(#expr,__FILE__,__LINE__,__func__))
+#else
+#define mi_assert(x)
+#endif
+
+#if (MI_DEBUG>1)
+#define mi_assert_internal    mi_assert
+#else
+#define mi_assert_internal(x)
+#endif
+
+#if (MI_DEBUG>2)
+#define mi_assert_expensive   mi_assert
+#else
+#define mi_assert_expensive(x)
+#endif
+
 
 // ------------------------------------------------------
 // Statistics
@@ -659,11 +681,11 @@
 #endif
 
 // add to stat keeping track of the peak
-void _mi_stat_increase(mi_stat_count_t* stat, size_t amount);
-void _mi_stat_decrease(mi_stat_count_t* stat, size_t amount);
-void _mi_stat_adjust_decrease(mi_stat_count_t* stat, size_t amount);
+mi_decl_internal void _mi_stat_increase(mi_stat_count_t* stat, size_t amount);
+mi_decl_internal void _mi_stat_decrease(mi_stat_count_t* stat, size_t amount);
+mi_decl_internal void _mi_stat_adjust_decrease(mi_stat_count_t* stat, size_t amount);
 // counters can just be increased
-void _mi_stat_counter_increase(mi_stat_counter_t* stat, size_t amount);
+mi_decl_internal void _mi_stat_counter_increase(mi_stat_counter_t* stat, size_t amount);
 
 #if (MI_STAT)
 #define mi_stat_increase(stat,amount)         _mi_stat_increase( &(stat), amount)
diff -urN a/mimalloc/include/mimalloc.h b/mimalloc/include/mimalloc.h
--- a/mimalloc/include/mimalloc.h	2025-07-15 11:19:20.286569503 -0400
+++ b/mimalloc/include/mimalloc.h	2025-07-15 11:19:32.332157478 -0400
@@ -60,7 +60,9 @@
   #define mi_attr_alloc_size2(s1,s2)
   #define mi_attr_alloc_align(p)
 #elif defined(__GNUC__)                 // includes clang and icc
-  #if defined(MI_SHARED_LIB) && defined(MI_SHARED_LIB_EXPORT)
+  #ifdef MI_LIBC_BUILD
+    #define mi_decl_export static
+  #elif defined(MI_SHARED_LIB) && defined(MI_SHARED_LIB_EXPORT)
     #define mi_decl_export              __attribute__((visibility("default")))
   #else
     #define mi_decl_export
diff -urN a/mimalloc/src/alloc-aligned.c b/mimalloc/src/alloc-aligned.c
--- a/mimalloc/src/alloc-aligned.c	2025-07-15 11:19:20.287254158 -0400
+++ b/mimalloc/src/alloc-aligned.c	2025-07-15 11:19:32.333341080 -0400
@@ -68,6 +68,7 @@
 #if MI_DEBUG > 0
       _mi_error_message(EOVERFLOW, "aligned allocation with a very large alignment cannot be used with an alignment offset (size %zu, alignment %zu, offset %zu)\n", size, alignment, offset);
 #endif
+      errno = EINVAL;
       return NULL;
     }
     oversize = (size <= MI_SMALL_SIZE_MAX ? MI_SMALL_SIZE_MAX + 1 /* ensure we use generic malloc path */ : size);
@@ -140,6 +141,7 @@
     #if MI_DEBUG > 0
     _mi_error_message(EOVERFLOW, "aligned allocation request is too large (size %zu, alignment %zu)\n", size, alignment);
     #endif
+    errno = ENOMEM;
     return NULL;
   }
 
@@ -173,6 +175,7 @@
     #if MI_DEBUG > 0
     _mi_error_message(EOVERFLOW, "aligned allocation requires the alignment to be a power-of-two (size %zu, alignment %zu)\n", size, alignment);
     #endif
+    errno = EINVAL;
     return NULL;
   }
 
diff -urN a/mimalloc/src/alloc.c b/mimalloc/src/alloc.c
--- a/mimalloc/src/alloc.c	2025-07-15 11:19:20.287421358 -0400
+++ b/mimalloc/src/alloc.c	2025-07-15 11:19:32.333589157 -0400
@@ -221,7 +221,7 @@
 
 mi_decl_nodiscard extern inline mi_decl_restrict void* mi_heap_calloc(mi_heap_t* heap, size_t count, size_t size) mi_attr_noexcept {
   size_t total;
-  if (mi_count_size_overflow(count,size,&total)) return NULL;
+  if (mi_count_size_overflow(count,size,&total)) { errno = ENOMEM; return NULL; }
   return mi_heap_zalloc(heap,total);
 }
 
@@ -480,7 +480,7 @@
 #else
 typedef void (*std_new_handler_t)(void);
 
-#if (defined(__GNUC__) || (defined(__clang__) && !defined(_MSC_VER)))  // exclude clang-cl, see issue #631
+#if !defined(MI_LIBC_BUILD) && (defined(__GNUC__) || (defined(__clang__) && !defined(_MSC_VER)))  // exclude clang-cl, see issue #631
 std_new_handler_t __attribute__((weak)) _ZSt15get_new_handlerv(void) {
   return NULL;
 }
diff -urN a/mimalloc/src/arena-abandon.c b/mimalloc/src/arena-abandon.c
--- a/mimalloc/src/arena-abandon.c	2025-07-15 11:19:20.287482330 -0400
+++ b/mimalloc/src/arena-abandon.c	2025-07-15 11:19:32.333671290 -0400
@@ -14,11 +14,11 @@
 #endif
 
 // Minimal exports for arena-abandoned.
-size_t      mi_arena_id_index(mi_arena_id_t id);
-mi_arena_t* mi_arena_from_index(size_t idx);
-size_t      mi_arena_get_count(void);
-void*       mi_arena_block_start(mi_arena_t* arena, mi_bitmap_index_t bindex);
-bool        mi_arena_memid_indices(mi_memid_t memid, size_t* arena_index, mi_bitmap_index_t* bitmap_index);
+mi_decl_internal size_t      mi_arena_id_index(mi_arena_id_t id);
+mi_decl_internal mi_arena_t* mi_arena_from_index(size_t idx);
+mi_decl_internal size_t      mi_arena_get_count(void);
+mi_decl_internal void*       mi_arena_block_start(mi_arena_t* arena, mi_bitmap_index_t bindex);
+mi_decl_internal bool        mi_arena_memid_indices(mi_memid_t memid, size_t* arena_index, mi_bitmap_index_t* bitmap_index);
 
 /* -----------------------------------------------------------
   Abandoned blocks/segments:
diff -urN a/mimalloc/src/arena.c b/mimalloc/src/arena.c
--- a/mimalloc/src/arena.c	2025-07-15 11:19:20.287541835 -0400
+++ b/mimalloc/src/arena.c	2025-07-15 11:19:32.333423563 -0400
@@ -99,7 +99,7 @@
   }
 }
 
-bool _mi_arena_memid_is_os_allocated(mi_memid_t memid) {
+static bool _mi_arena_memid_is_os_allocated(mi_memid_t memid) {
   return (memid.memkind == MI_MEM_OS);
 }
 
diff -urN a/mimalloc/src/bitmap.h b/mimalloc/src/bitmap.h
--- a/mimalloc/src/bitmap.h	2025-07-15 11:19:20.287671810 -0400
+++ b/mimalloc/src/bitmap.h	2025-07-15 11:19:32.333891989 -0400
@@ -70,30 +70,30 @@
 
 // Try to atomically claim a sequence of `count` bits in a single
 // field at `idx` in `bitmap`. Returns `true` on success.
-bool _mi_bitmap_try_find_claim_field(mi_bitmap_t bitmap, size_t idx, const size_t count, mi_bitmap_index_t* bitmap_idx);
+mi_decl_internal bool _mi_bitmap_try_find_claim_field(mi_bitmap_t bitmap, size_t idx, const size_t count, mi_bitmap_index_t* bitmap_idx);
 
 // Starts at idx, and wraps around to search in all `bitmap_fields` fields.
 // For now, `count` can be at most MI_BITMAP_FIELD_BITS and will never cross fields.
-bool _mi_bitmap_try_find_from_claim(mi_bitmap_t bitmap, const size_t bitmap_fields, const size_t start_field_idx, const size_t count, mi_bitmap_index_t* bitmap_idx);
+mi_decl_internal bool _mi_bitmap_try_find_from_claim(mi_bitmap_t bitmap, const size_t bitmap_fields, const size_t start_field_idx, const size_t count, mi_bitmap_index_t* bitmap_idx);
 
 // Like _mi_bitmap_try_find_from_claim but with an extra predicate that must be fullfilled
 typedef bool (mi_cdecl *mi_bitmap_pred_fun_t)(mi_bitmap_index_t bitmap_idx, void* pred_arg);
-bool _mi_bitmap_try_find_from_claim_pred(mi_bitmap_t bitmap, const size_t bitmap_fields, const size_t start_field_idx, const size_t count, mi_bitmap_pred_fun_t pred_fun, void* pred_arg, mi_bitmap_index_t* bitmap_idx);
+mi_decl_internal bool _mi_bitmap_try_find_from_claim_pred(mi_bitmap_t bitmap, const size_t bitmap_fields, const size_t start_field_idx, const size_t count, mi_bitmap_pred_fun_t pred_fun, void* pred_arg, mi_bitmap_index_t* bitmap_idx);
 
 // Set `count` bits at `bitmap_idx` to 0 atomically
 // Returns `true` if all `count` bits were 1 previously.
-bool _mi_bitmap_unclaim(mi_bitmap_t bitmap, size_t bitmap_fields, size_t count, mi_bitmap_index_t bitmap_idx);
+mi_decl_internal bool _mi_bitmap_unclaim(mi_bitmap_t bitmap, size_t bitmap_fields, size_t count, mi_bitmap_index_t bitmap_idx);
 
 // Try to set `count` bits at `bitmap_idx` from 0 to 1 atomically. 
 // Returns `true` if successful when all previous `count` bits were 0.
-bool _mi_bitmap_try_claim(mi_bitmap_t bitmap, size_t bitmap_fields, size_t count, mi_bitmap_index_t bitmap_idx);
+mi_decl_internal bool _mi_bitmap_try_claim(mi_bitmap_t bitmap, size_t bitmap_fields, size_t count, mi_bitmap_index_t bitmap_idx);
 
 // Set `count` bits at `bitmap_idx` to 1 atomically
 // Returns `true` if all `count` bits were 0 previously. `any_zero` is `true` if there was at least one zero bit.
-bool _mi_bitmap_claim(mi_bitmap_t bitmap, size_t bitmap_fields, size_t count, mi_bitmap_index_t bitmap_idx, bool* any_zero);
+mi_decl_internal bool _mi_bitmap_claim(mi_bitmap_t bitmap, size_t bitmap_fields, size_t count, mi_bitmap_index_t bitmap_idx, bool* any_zero);
 
-bool _mi_bitmap_is_claimed(mi_bitmap_t bitmap, size_t bitmap_fields, size_t count, mi_bitmap_index_t bitmap_idx);
-bool _mi_bitmap_is_any_claimed(mi_bitmap_t bitmap, size_t bitmap_fields, size_t count, mi_bitmap_index_t bitmap_idx);
+mi_decl_internal bool _mi_bitmap_is_claimed(mi_bitmap_t bitmap, size_t bitmap_fields, size_t count, mi_bitmap_index_t bitmap_idx);
+mi_decl_internal bool _mi_bitmap_is_any_claimed(mi_bitmap_t bitmap, size_t bitmap_fields, size_t count, mi_bitmap_index_t bitmap_idx);
 
 
 //--------------------------------------------------------------------------
@@ -103,17 +103,17 @@
 
 // Find `count` bits of zeros and set them to 1 atomically; returns `true` on success.
 // Starts at idx, and wraps around to search in all `bitmap_fields` fields.
-bool _mi_bitmap_try_find_from_claim_across(mi_bitmap_t bitmap, const size_t bitmap_fields, const size_t start_field_idx, const size_t count, mi_bitmap_index_t* bitmap_idx);
+mi_decl_internal bool _mi_bitmap_try_find_from_claim_across(mi_bitmap_t bitmap, const size_t bitmap_fields, const size_t start_field_idx, const size_t count, mi_bitmap_index_t* bitmap_idx);
 
 // Set `count` bits at `bitmap_idx` to 0 atomically
 // Returns `true` if all `count` bits were 1 previously.
-bool _mi_bitmap_unclaim_across(mi_bitmap_t bitmap, size_t bitmap_fields, size_t count, mi_bitmap_index_t bitmap_idx);
+mi_decl_internal bool _mi_bitmap_unclaim_across(mi_bitmap_t bitmap, size_t bitmap_fields, size_t count, mi_bitmap_index_t bitmap_idx);
 
 // Set `count` bits at `bitmap_idx` to 1 atomically
 // Returns `true` if all `count` bits were 0 previously. `any_zero` is `true` if there was at least one zero bit.
-bool _mi_bitmap_claim_across(mi_bitmap_t bitmap, size_t bitmap_fields, size_t count, mi_bitmap_index_t bitmap_idx, bool* pany_zero, size_t* already_set);
+mi_decl_internal bool _mi_bitmap_claim_across(mi_bitmap_t bitmap, size_t bitmap_fields, size_t count, mi_bitmap_index_t bitmap_idx, bool* pany_zero, size_t* already_set);
 
-bool _mi_bitmap_is_claimed_across(mi_bitmap_t bitmap, size_t bitmap_fields, size_t count, mi_bitmap_index_t bitmap_idx, size_t* already_set);
-bool _mi_bitmap_is_any_claimed_across(mi_bitmap_t bitmap, size_t bitmap_fields, size_t count, mi_bitmap_index_t bitmap_idx);
+mi_decl_internal bool _mi_bitmap_is_claimed_across(mi_bitmap_t bitmap, size_t bitmap_fields, size_t count, mi_bitmap_index_t bitmap_idx, size_t* already_set);
+mi_decl_internal bool _mi_bitmap_is_any_claimed_across(mi_bitmap_t bitmap, size_t bitmap_fields, size_t count, mi_bitmap_index_t bitmap_idx);
 
 #endif
diff -urN a/mimalloc/src/init.c b/mimalloc/src/init.c
--- a/mimalloc/src/init.c	2025-07-15 11:19:20.287837683 -0400
+++ b/mimalloc/src/init.c	2025-07-15 11:19:32.334034396 -0400
@@ -13,6 +13,11 @@
 
 
 // Empty page used to initialize the small free pages array
+
+#ifdef MI_LIBC_BUILD
+static
+#endif
+
 const mi_page_t _mi_page_empty = {
   0,
   false, false, false, false,
@@ -146,7 +151,10 @@
 }
 
 // the thread-local default heap for allocation
+
+#ifndef MI_LIBC_BUILD
 mi_decl_thread mi_heap_t* _mi_heap_default = (mi_heap_t*)&_mi_heap_empty;
+#endif
 
 extern mi_decl_hidden mi_heap_t _mi_heap_main;
 
@@ -178,8 +186,14 @@
   MI_PAGE_QUEUES_EMPTY
 };
 
+#ifdef MI_LIBC_BUILD
+static
+#endif
 bool _mi_process_is_initialized = false;  // set to `true` in `mi_process_init`.
 
+#ifdef MI_LIBC_BUILD
+static
+#endif
 mi_stats_t _mi_stats_main = { MI_STAT_VERSION, MI_STATS_NULL };
 
 #if MI_GUARDED
@@ -548,6 +562,9 @@
   *mi_prim_tls_pthread_heap_slot() = heap;
   #elif defined(MI_TLS_PTHREAD)
   // we use _mi_heap_default_key
+  #elif defined(MI_LIBC_BUILD)
+  // chimera musl
+  __pthread_self()->malloc_tls = heap;
   #else
   _mi_heap_default = heap;
   #endif
@@ -579,7 +596,7 @@
 // Called once by the process loader from `src/prim/prim.c`
 void _mi_auto_process_init(void) {
   mi_heap_main_init();
-  #if defined(__APPLE__) || defined(MI_TLS_RECURSE_GUARD)
+  #if !defined(MI_LIBC_BUILD) && (defined(__APPLE__) || defined(MI_TLS_RECURSE_GUARD))
   volatile mi_heap_t* dummy = _mi_heap_default; // access TLS to allocate it before setting tls_initialized to true;
   if (dummy == NULL) return;                    // use dummy or otherwise the access may get optimized away (issue #697)
   #endif
diff -urN a/mimalloc/src/mimalloc.c b/mimalloc/src/mimalloc.c
--- a/mimalloc/src/mimalloc.c	1969-12-31 19:00:00.000000000 -0500
+++ b/mimalloc/src/mimalloc.c	2025-07-15 11:19:32.334395755 -0400
@@ -0,0 +1,115 @@
+/* The Chimera Linux unified mimalloc configuration. */
+
+/* enable our changes */
+#define MI_LIBC_BUILD 1
+/* the libc malloc should not read any env vars */
+#define MI_NO_GETENV 1
+/* this is a hardened build */
+#define MI_SECURE 4
+/* this would be nice to have, but unfortunately it
+ * makes some things a lot slower (e.g. sort(1) becomes
+ * roughly 2.5x slower) so disable unless we figure out
+ * some way to make it acceptable...
+ */
+#define MI_PADDING 0
+
+/* use smaller segments to accommodate smaller arenas */
+#define MI_SEGMENT_SHIFT (7 + MI_SEGMENT_SLICE_SHIFT)
+
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wunused-function"
+
+#include <features.h>
+/* small workaround for musl includes */
+#ifdef weak
+#undef weak
+#endif
+
+#include "pthread_impl.h"
+
+/* since we are internal we can make syscalls more direct (via macros) */
+#include "syscall.h"
+#define madvise __madvise
+#define MADV_DONTNEED POSIX_MADV_DONTNEED
+
+/* some verification whether we can make a valid build */
+#include <stdatomic.h>
+
+#if ATOMIC_LONG_LOCK_FREE != 2 || ATOMIC_CHAR_LOCK_FREE != 2
+#error Words and bytes must always be lock-free in this context
+#endif
+
+/* arena purge timing stuff (may fix later), stats (can patch out) */
+#if ATOMIC_LLONG_LOCK_FREE != 2
+#error 64-bit atomics must be lock-free for now
+#endif
+
+/* the whole mimalloc source */
+#include "static.c"
+
+/* chimera entrypoints */
+
+#define INTERFACE __attribute__((visibility("default")))
+
+extern int __malloc_replaced;
+extern int __aligned_alloc_replaced;
+
+void * const __malloc_tls_default = (void *)&_mi_heap_empty;
+
+void __malloc_init(pthread_t p) {
+    _mi_auto_process_init();
+}
+
+void __malloc_tls_teardown(pthread_t p) {
+    /* if we never allocated on it, don't do anything */
+    if (p->malloc_tls == (void *)&_mi_heap_empty)
+        return;
+    /* otherwise finalize the thread and reset */
+    _mi_thread_done(p->malloc_tls);
+    p->malloc_tls = (void *)&_mi_heap_empty;
+}
+
+/* we have nothing to do here, mimalloc is lock-free */
+void __malloc_atfork(int who) {
+    if (who < 0) {
+        /* disable */
+    } else {
+        /* enable */
+    }
+}
+
+/* we have no way to implement this AFAICT */
+void __malloc_donate(char *a, char *b) { (void)a; (void)b; }
+
+void *__libc_calloc(size_t m, size_t n) {
+    return mi_calloc(m, n);
+}
+
+void __libc_free(void *ptr) {
+    mi_free(ptr);
+}
+
+void *__libc_malloc_impl(size_t len) {
+    return mi_malloc(len);
+}
+
+void *__libc_realloc(void *ptr, size_t len) {
+    return mi_realloc(ptr, len);
+}
+
+/* technically mi_aligned_alloc and mi_memalign are the same in mimalloc
+ * which is good for us because musl implements memalign with aligned_alloc
+ */
+INTERFACE void *aligned_alloc(size_t align, size_t len) {
+    if (mi_unlikely(__malloc_replaced && !__aligned_alloc_replaced)) {
+        errno = ENOMEM;
+        return NULL;
+    }
+    void *p = mi_malloc_aligned(len, align);
+    mi_assert_internal(((uintptr_t)p % align) == 0);
+    return p;
+}
+
+INTERFACE size_t malloc_usable_size(void *p) {
+    return mi_usable_size(p);
+}
diff -urN a/mimalloc/src/options.c b/mimalloc/src/options.c
--- a/mimalloc/src/options.c	2025-07-15 11:19:20.287960185 -0400
+++ b/mimalloc/src/options.c	2025-07-15 11:19:32.336234400 -0400
@@ -12,8 +12,6 @@
 #include <stdio.h>      // stdin/stdout
 #include <stdlib.h>     // abort
 
-
-
 static long mi_max_error_count   = 16; // stop outputting errors after this (use < 0 for no limit)
 static long mi_max_warning_count = 16; // stop outputting warnings after this (use < 0 for no limit)
 
@@ -68,9 +66,9 @@
 // in KiB
 #ifndef MI_DEFAULT_ARENA_RESERVE
  #if (MI_INTPTR_SIZE>4)
-  #define MI_DEFAULT_ARENA_RESERVE 1024L*1024L
+  #define MI_DEFAULT_ARENA_RESERVE 64L*1024L
  #else
-  #define MI_DEFAULT_ARENA_RESERVE 128L*1024L
+  #define MI_DEFAULT_ARENA_RESERVE 64L*1024L
  #endif
 #endif
 
@@ -354,7 +352,7 @@
   if (count>MI_MAX_DELAY_OUTPUT) count = MI_MAX_DELAY_OUTPUT;
   out_buf[count] = 0;
   out(out_buf,arg);
-  if (!no_more_buf) {
+  if (mi_option_is_enabled(mi_option_verbose) && !no_more_buf) {
     out_buf[count] = '\n'; // if continue with the buffer, insert a newline
   }
 }
@@ -403,6 +401,7 @@
 static _Atomic(size_t) error_count;   // = 0;  // when >= max_error_count stop emitting errors
 static _Atomic(size_t) warning_count; // = 0;  // when >= max_warning_count stop emitting warnings
 
+#ifndef MI_LIBC_BUILD
 // When overriding malloc, we may recurse into mi_vfprintf if an allocation
 // inside the C runtime causes another message.
 // In some cases (like on macOS) the loader already allocates which
@@ -424,6 +423,14 @@
   recurse = false;
 }
 
+#else
+// We don't really care because from a libc, we cannot override
+// the output functions (so there is no chance of recursive alloc)
+// and we get to avoid a thread-local thing this way
+static bool mi_recurse_enter_prim(void) { return true; }
+static void mi_recurse_exit_prim(void) {}
+#endif
+
 static bool mi_recurse_enter(void) {
   #if defined(__APPLE__) || defined(__ANDROID__) || defined(MI_TLS_RECURSE_GUARD)
   if (_mi_preloading()) return false;
diff -urN a/mimalloc/src/os.c b/mimalloc/src/os.c
--- a/mimalloc/src/os.c	2025-07-15 11:19:20.288022623 -0400
+++ b/mimalloc/src/os.c	2025-07-15 11:19:32.334450511 -0400
@@ -329,7 +329,7 @@
   void* p = mi_os_prim_alloc(size, 0, true, false, &os_is_large, &os_is_zero);
   if (p == NULL) return NULL;
 
-  *memid = _mi_memid_create_os(p, size, true, os_is_zero, os_is_large);  
+  *memid = _mi_memid_create_os(p, size, true, os_is_zero, os_is_large);
   mi_assert_internal(memid->mem.os.size >= size);
   mi_assert_internal(memid->initially_committed);
   return p;
@@ -355,7 +355,7 @@
 
   mi_assert_internal(memid->mem.os.size >= size);
   mi_assert_internal(_mi_is_aligned(p,alignment));
-  if (commit) { mi_assert_internal(memid->initially_committed); }  
+  if (commit) { mi_assert_internal(memid->initially_committed); }
   return p;
 }
 
@@ -527,7 +527,6 @@
   return (err == 0);
 }
 
-
 void _mi_os_reuse( void* addr, size_t size ) {
   // page align conservatively within the range
   size_t csize = 0;
@@ -729,7 +728,7 @@
 Support NUMA aware allocation
 -----------------------------------------------------------------------------*/
 
-static _Atomic(size_t) mi_numa_node_count; // = 0   // cache the node count
+static _Atomic(size_t)  mi_numa_node_count; // = 0   // cache the node count
 
 int _mi_os_numa_node_count(void) {
   size_t count = mi_atomic_load_acquire(&mi_numa_node_count);
diff -urN a/mimalloc/src/page.c b/mimalloc/src/page.c
--- a/mimalloc/src/page.c	2025-07-15 11:19:20.288138769 -0400
+++ b/mimalloc/src/page.c	2025-07-15 11:19:32.334557508 -0400
@@ -112,7 +112,7 @@
   return true;
 }
 
-extern mi_decl_hidden bool _mi_process_is_initialized;             // has mi_process_init been called?
+mi_decl_internal bool _mi_process_is_initialized;             // has mi_process_init been called?
 
 bool _mi_page_is_valid(mi_page_t* page) {
   mi_assert_internal(mi_page_is_valid_init(page));
@@ -445,7 +445,7 @@
   mi_segments_tld_t* segments_tld = &heap->tld->segments;
   mi_page_queue_remove(pq, page);
 
-  // and free it  
+  // and free it
   mi_page_set_heap(page,NULL);
   _mi_segment_page_free(page, force, segments_tld);
 }
@@ -640,7 +640,6 @@
   if (page->free != NULL) return true;
   #endif
   if (page->capacity >= page->reserved) return true;
-
   mi_stat_counter_increase(tld->stats.pages_extended, 1);
 
   // calculate the extend count
@@ -965,6 +964,7 @@
   if mi_unlikely(req_size > (MI_MEDIUM_OBJ_SIZE_MAX - MI_PADDING_SIZE) || huge_alignment > 0) {
     if mi_unlikely(req_size > MI_MAX_ALLOC_SIZE) {
       _mi_error_message(EOVERFLOW, "allocation request is too large (%zu bytes)\n", req_size);
+      errno = ENOMEM;
       return NULL;
     }
     else {
@@ -1023,6 +1023,7 @@
   if mi_unlikely(page == NULL) { // out of memory
     const size_t req_size = size - MI_PADDING_SIZE;  // correct for padding_size in case of an overflow on `size`
     _mi_error_message(ENOMEM, "unable to allocate memory (%zu bytes)\n", req_size);
+    errno = ENOMEM;
     return NULL;
   }
 
diff -urN a/mimalloc/src/prim/unix/prim.c b/mimalloc/src/prim/unix/prim.c
--- a/mimalloc/src/prim/unix/prim.c	2025-07-15 11:19:20.288659717 -0400
+++ b/mimalloc/src/prim/unix/prim.c	2025-07-15 11:19:32.335063718 -0400
@@ -241,7 +241,8 @@
     // fall back to regular mmap
   }
   #endif
-  #if (MI_INTPTR_SIZE >= 8) && !defined(MAP_ALIGNED)
+  // we cannot do this for our libc allocator as it results in early map with range that conflicts with asan
+  #if (MI_INTPTR_SIZE >= 8) && !defined(MAP_ALIGNED) && !defined(MI_LIBC_BUILD)
   // on 64-bit systems, use the virtual address area after 2TiB for 4MiB aligned allocations
   if (addr == NULL) {
     void* hint = _mi_os_get_aligned_hint(try_alignment, size);
@@ -387,6 +388,7 @@
   mi_assert_internal(size > 0 && (size % _mi_os_page_size()) == 0);
   mi_assert_internal(commit || !allow_large);
   mi_assert_internal(try_alignment > 0);
+
   if (hint_addr == NULL && size >= 8*MI_UNIX_LARGE_PAGE_SIZE && try_alignment > 1 && _mi_is_power_of_two(try_alignment) && try_alignment < MI_UNIX_LARGE_PAGE_SIZE) {
     try_alignment = MI_UNIX_LARGE_PAGE_SIZE; // try to align along large page size for larger allocations
   }
@@ -440,11 +442,11 @@
 int _mi_prim_decommit(void* start, size_t size, bool* needs_recommit) {
   int err = 0;
   #if defined(__APPLE__) && defined(MADV_FREE_REUSABLE)
-    // decommit on macOS: use MADV_FREE_REUSABLE as it does immediate rss accounting (issue #1097)
+    // decommit on macOS: use MADV_FREE_REUSABLE as it does immediate rss accountin>
     err = unix_madvise(start, size, MADV_FREE_REUSABLE);
     if (err) { err = unix_madvise(start, size, MADV_DONTNEED); }
   #else
-    // decommit: use MADV_DONTNEED as it decreases rss immediately (unlike MADV_FREE)
+    // decommit: use MADV_DONTNEED as it decreases rss immediately (unlike MADV_FRE>
     err = unix_madvise(start, size, MADV_DONTNEED);
   #endif  
   #if !MI_DEBUG && MI_SECURE<=2
@@ -466,9 +468,9 @@
 int _mi_prim_reset(void* start, size_t size) {
   int err = 0;
 
-  // on macOS can use MADV_FREE_REUSABLE (but we disable this for now as it seems slower)
-  #if 0 && defined(__APPLE__) && defined(MADV_FREE_REUSABLE) 
-  err = unix_madvise(start, size, MADV_FREE_REUSABLE);  
+  // on macOS can use MADV_FREE_REUSABLE (but we disable this for now as it seems s>
+  #if 0 && defined(__APPLE__) && defined(MADV_FREE_REUSABLE)
+  err = unix_madvise(start, size, MADV_FREE_REUSABLE);
   if (err==0) return 0;
   // fall through
   #endif
@@ -888,7 +890,7 @@
 // Thread init/done
 //----------------------------------------------------------------
 
-#if defined(MI_USE_PTHREADS)
+#if defined(MI_USE_PTHREADS) && !defined(MI_LIBC_BUILD)
 
 // use pthread local storage keys to detect thread ending
 // (and used with MI_TLS_PTHREADS for the default heap)
diff -urN a/mimalloc-verify-syms.sh b/mimalloc-verify-syms.sh
--- a/mimalloc-verify-syms.sh	1969-12-31 19:00:00.000000000 -0500
+++ b/mimalloc-verify-syms.sh	2025-07-15 11:19:32.337605876 -0400
@@ -0,0 +1,18 @@
+#!/bin/sh
+
+nm "$1" | grep '[0-9A-Za-z] [A-Z] ' | while read -r addr type name; do
+    case "$name" in
+        # glue symbols
+        __libc_*|__malloc_*) ;;
+        # compiler-generated
+        .L*) ;;
+        # directly provided api
+        aligned_alloc|malloc_usable_size) ;;
+        # mimalloc heaps
+        _mi_heap_empty|_mi_heap_main) ;;
+        *)
+            echo "unexpected symbol $name ($type)"
+            exit 1
+            ;;
+    esac
+done
diff -urN a/src/env/__init_tls.c b/src/env/__init_tls.c
--- a/src/env/__init_tls.c	2025-07-15 11:19:20.134321789 -0400
+++ b/src/env/__init_tls.c	2025-07-15 11:19:32.346943184 -0400
@@ -24,6 +24,7 @@
 	td->robust_list.head = &td->robust_list.head;
 	td->sysinfo = __sysinfo;
 	td->next = td->prev = td;
+	td->malloc_tls = __malloc_tls_default;
 	return 0;
 }
 
@@ -86,6 +87,7 @@
 	Phdr *phdr, *tls_phdr=0;
 	size_t base = 0;
 	void *mem;
+	pthread_t self;
 
 	for (p=(void *)aux[AT_PHDR],n=aux[AT_PHNUM]; n; n--,p+=aux[AT_PHENT]) {
 		phdr = (void *)p;
@@ -146,8 +148,11 @@
 	}
 
 	/* Failure to initialize thread pointer is always fatal. */
-	if (__init_tp(__copy_tls(mem)) < 0)
+	self = __copy_tls(mem);
+	if (__init_tp(self) < 0)
 		a_crash();
+	/* Initialize malloc. */
+	__malloc_init(self);
 }
 
 weak_alias(static_init_tls, __init_tls);
diff -urN a/src/exit/exit.c b/src/exit/exit.c
--- a/src/exit/exit.c	2025-07-15 11:19:20.135223232 -0400
+++ b/src/exit/exit.c	2025-07-15 11:19:32.348242793 -0400
@@ -1,6 +1,7 @@
 #include <stdlib.h>
 #include <stdint.h>
 #include "libc.h"
+#include "pthread_impl.h"
 
 static void dummy()
 {
@@ -20,6 +21,7 @@
 	for (; a>(uintptr_t)&__fini_array_start; a-=sizeof(void(*)()))
 		(*(void (**)())(a-sizeof(void(*)())))();
 	_fini();
+	__malloc_tls_teardown(__pthread_self());
 }
 
 weak_alias(libc_exit_fini, __libc_exit_fini);
diff -urN a/src/internal/pthread_impl.h b/src/internal/pthread_impl.h
--- a/src/internal/pthread_impl.h	2025-07-15 11:19:20.139366435 -0400
+++ b/src/internal/pthread_impl.h	2025-07-15 11:19:32.352795894 -0400
@@ -6,10 +6,15 @@
 #include <errno.h>
 #include <limits.h>
 #include <sys/mman.h>
+#ifndef MI_LIBC_BUILD
 #include "libc.h"
 #include "syscall.h"
 #include "atomic.h"
 #include "futex.h"
+#else
+/* restricted version for MI_LIBC_BUILD; need struct pthread + __pthread_self */
+#include <stdint.h>
+#endif
 
 #include "pthread_arch.h"
 
@@ -58,6 +63,7 @@
 	volatile int killlock[1];
 	char *dlerror_buf;
 	void *stdio_locks;
+	void *malloc_tls;
 
 	/* Part 3 -- the positions of these fields relative to
 	 * the end of the structure is external and internal ABI. */
@@ -122,6 +128,7 @@
 #define __pthread_self() ((pthread_t)__get_tp())
 #endif
 
+#ifndef MI_LIBC_BUILD
 #ifndef tls_mod_off_t
 #define tls_mod_off_t size_t
 #endif
@@ -187,6 +194,11 @@
 hidden void __tl_unlock(void);
 hidden void __tl_sync(pthread_t);
 
+extern hidden void * const __malloc_tls_default;
+
+hidden void __malloc_init(pthread_t);
+hidden void __malloc_tls_teardown(pthread_t);
+
 extern hidden volatile int __thread_list_lock;
 
 extern hidden volatile int __abort_lock[1];
@@ -201,5 +213,5 @@
 #define DEFAULT_GUARD_MAX (1<<20)
 
 #define __ATTRP_C11_THREAD ((void*)(uintptr_t)-1)
-
+#endif
 #endif
diff -urN a/src/ldso/loongarch64/tlsdesc.s b/src/ldso/loongarch64/tlsdesc.s
--- a/src/ldso/loongarch64/tlsdesc.s	1969-12-31 19:00:00.000000000 -0500
+++ b/src/ldso/loongarch64/tlsdesc.s	2025-07-15 11:19:32.355007912 -0400
@@ -0,0 +1,37 @@
+.text
+.global __tlsdesc_static
+.hidden __tlsdesc_static
+.type __tlsdesc_static,%function
+__tlsdesc_static:
+	ld.d $a0, $a0, 8
+	jr $ra
+# size_t __tlsdesc_dynamic(size_t *a)
+# {
+#      struct {size_t modidx,off;} *p = (void*)a[1];
+#      size_t *dtv = *(size_t**)(tp - 8);
+#      return dtv[p->modidx]  p->off - tp;
+# }
+.global __tlsdesc_dynamic
+.hidden __tlsdesc_dynamic
+.type __tlsdesc_dynamic,%function
+__tlsdesc_dynamic:
+	addi.d $sp, $sp, -16
+	st.d $t1, $sp, 0
+	st.d $t2, $sp, 8
+
+	ld.d $t2, $tp, -8 # t2=dtv
+
+	ld.d $a0, $a0, 8  # a0=&{modidx,off}
+	ld.d $t1, $a0, 8  # t1=off
+	ld.d $a0, $a0, 0  # a0=modidx
+	slli.d $a0, $a0, 3  # a0=8*modidx
+
+	add.d $a0, $a0, $t2  # a0=dtv8*modidx
+	ld.d $a0, $a0, 0  # a0=dtv[modidx]
+	add.d $a0, $a0, $t1 # a0=dtv[modidx]off
+	sub.d $a0, $a0, $tp # a0=dtv[modidx]off-tp
+
+	ld.d $t1, $sp, 0
+	ld.d $t2, $sp, 8
+	addi.d $sp, $sp, 16
+	jr $ra
diff -urN a/src/locale/bind_textdomain_codeset.c b/src/locale/bind_textdomain_codeset.c
--- a/src/locale/bind_textdomain_codeset.c	2025-07-15 11:19:20.146546970 -0400
+++ b/src/locale/bind_textdomain_codeset.c	2025-07-15 11:19:32.362168123 -0400
@@ -5,7 +5,9 @@
 
 char *bind_textdomain_codeset(const char *domainname, const char *codeset)
 {
-	if (codeset && strcasecmp(codeset, "UTF-8"))
+	if (codeset && strcasecmp(codeset, "UTF-8")) {
 		errno = EINVAL;
-	return NULL;
+		return 0;
+	}
+	return "UTF-8";
 }
diff -urN a/src/locale/iconv.c b/src/locale/iconv.c
--- a/src/locale/iconv.c	2025-07-15 11:19:20.147171282 -0400
+++ b/src/locale/iconv.c	2025-07-15 11:19:32.363300113 -0400
@@ -495,7 +495,7 @@
 			if (c >= 93 || d >= 94) {
 				c += (0xa1-0x81);
 				d += 0xa1;
-				if (c >= 93 || c>=0xc6-0x81 && d>0x52)
+				if (c > 0xc6-0x81 || c==0xc6-0x81 && d>0x52)
 					goto ilseq;
 				if (d-'A'<26) d = d-'A';
 				else if (d-'a'<26) d = d-'a'+26;
@@ -538,6 +538,10 @@
 				if (*outb < k) goto toobig;
 				memcpy(*out, tmp, k);
 			} else k = wctomb_utf8(*out, c);
+			/* This failure condition should be unreachable, but
+			 * is included to prevent decoder bugs from translating
+			 * into advancement outside the output buffer range. */
+			if (k>4) goto ilseq;
 			*out += k;
 			*outb -= k;
 			break;
diff -urN a/src/locale/locale_map.c b/src/locale/locale_map.c
--- a/src/locale/locale_map.c	2025-07-15 11:19:20.147538508 -0400
+++ b/src/locale/locale_map.c	2025-07-15 11:19:32.363740113 -0400
@@ -64,8 +64,10 @@
 		if (!strcmp(val, p->name)) return p;
 
 	if (!libc.secure) path = getenv("MUSL_LOCPATH");
-	/* FIXME: add a default path? */
-
+	if (!path) {
+		// Provided by musl-locales
+		path = "/usr/share/i18n/locales/musl";
+	}
 	if (path) for (; *path; path=z+!!*z) {
 		z = __strchrnul(path, ':');
 		l = z - path;
diff -urN a/src/malloc/calloc.c b/src/malloc/calloc.c
--- a/src/malloc/calloc.c	2025-07-15 11:19:20.148246421 -0400
+++ b/src/malloc/calloc.c	2025-07-15 11:19:32.364578979 -0400
@@ -32,6 +32,10 @@
 
 void *calloc(size_t m, size_t n)
 {
+#ifdef LIBC_CALLOC_EXTERNAL
+	if (!__malloc_replaced)
+		return __libc_calloc(m, n);
+#endif
 	if (n && m > (size_t)-1/n) {
 		errno = ENOMEM;
 		return 0;
diff -urN a/src/malloc/external/empty.h b/src/malloc/external/empty.h
--- a/src/malloc/external/empty.h	1969-12-31 19:00:00.000000000 -0500
+++ b/src/malloc/external/empty.h	2025-07-15 11:19:32.364717823 -0400
@@ -0,0 +1 @@
+/* empty */
diff -urN a/src/malloc/libc_calloc.c b/src/malloc/libc_calloc.c
--- a/src/malloc/libc_calloc.c	2025-07-15 11:19:20.148331208 -0400
+++ b/src/malloc/libc_calloc.c	2025-07-15 11:19:32.364815881 -0400
@@ -1,4 +1,8 @@
+#ifndef LIBC_CALLOC_EXTERNAL
+
 #define calloc __libc_calloc
 #define malloc __libc_malloc
 
 #include "calloc.c"
+
+#endif
diff -urN a/src/malloc/mallocng/malloc.c b/src/malloc/mallocng/malloc.c
--- a/src/malloc/mallocng/malloc.c	2025-07-15 11:19:20.148601355 -0400
+++ b/src/malloc/mallocng/malloc.c	2025-07-15 11:19:32.365134148 -0400
@@ -7,6 +7,8 @@
 
 #include "meta.h"
 
+hidden void * const __malloc_tls_default = NULL;
+
 LOCK_OBJ_DEF;
 
 const uint16_t size_classes[] = {
diff -urN a/src/network/res_msend.c b/src/network/res_msend.c
--- a/src/network/res_msend.c	2025-07-15 11:19:20.179532859 -0400
+++ b/src/network/res_msend.c	2025-07-15 11:19:32.418773610 -0400
@@ -83,9 +83,9 @@
 	int fd;
 	int timeout, attempts, retry_interval, servfail_retry;
 	union {
-		struct sockaddr_in sin;
 		struct sockaddr_in6 sin6;
-	} sa = {0}, ns[MAXNS] = {{0}};
+		struct sockaddr_in sin;
+	} sa = {}, ns[MAXNS] = {{}};
 	socklen_t sl = sizeof sa.sin;
 	int nns = 0;
 	int family = AF_INET;
diff -urN a/src/string/x86_64/memmove.s b/src/string/x86_64/memmove.s
--- a/src/string/x86_64/memmove.s	2025-07-15 11:19:20.204323837 -0400
+++ b/src/string/x86_64/memmove.s	2025-07-15 11:19:32.523801545 -0400
@@ -1,3 +1,25 @@
+.hidden __memcpy_fwd
+__memcpy_fwd:
+	mov %rdi,%rax
+	cmp $8,%rdx
+	jc 1f
+	test $7,%edi
+	jz 1f
+2:	movsb
+	dec %rdx
+	test $7,%edi
+	jnz 2b
+1:	mov %rdx,%rcx
+	shr $3,%rcx
+	rep
+	movsq
+	and $7,%edx
+	jz 1f
+2:	movsb
+	dec %edx
+	jnz 2b
+1:	ret
+
 .global memmove
 .type memmove,@function
 memmove:
diff -urN a/src/thread/pthread_create.c b/src/thread/pthread_create.c
--- a/src/thread/pthread_create.c	2025-07-15 11:19:20.251091462 -0400
+++ b/src/thread/pthread_create.c	2025-07-15 11:19:32.549745744 -0400
@@ -68,6 +68,7 @@
 	}
 
 	__pthread_tsd_run_dtors();
+	__malloc_tls_teardown(self);
 
 	__block_app_sigs(&set);
 
@@ -319,6 +320,7 @@
 	new->self = new;
 	new->tsd = (void *)tsd;
 	new->locale = &libc.global_locale;
+	new->malloc_tls = __malloc_tls_default;
 	if (attr._a_detach) {
 		new->detach_state = DT_DETACHED;
 	} else {
@@ -395,3 +397,8 @@
 
 weak_alias(__pthread_exit, pthread_exit);
 weak_alias(__pthread_create, pthread_create);
+
+static void mdummy(pthread_t p) {}
+
+weak_alias(mdummy, __malloc_init);
+weak_alias(mdummy, __malloc_tls_teardown);
